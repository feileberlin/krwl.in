name: ‚öôÔ∏è Configure Scraper (GitHub UI)

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action to perform'
        required: true
        type: choice
        options:
          - 'Add New Source'
          - 'Edit Source'
          - 'Test Source'
          - 'Enable/Disable Source'
          - 'List All Sources'
          - 'Fix Broken Scrapers'
      
      source_name:
        description: 'Source name (for Add/Edit/Test)'
        required: false
        type: string
      
      source_type:
        description: 'Source type (for Add)'
        required: false
        type: choice
        options:
          - 'rss'
          - 'html'
          - 'api'
          - 'atom'
          - 'facebook'
          - 'instagram'
          - 'tiktok'
          - 'x'
          - 'telegram'
      
      source_url:
        description: 'Source URL (for Add/Edit)'
        required: false
        type: string
      
      source_enabled:
        description: 'Enable source? (for Add/Edit)'
        required: false
        type: boolean
        default: true
      
      filter_ads:
        description: 'Filter ads/spam?'
        required: false
        type: boolean
        default: false
      
      exclude_keywords:
        description: 'Exclude keywords (comma-separated)'
        required: false
        type: string
      
      max_days_ahead:
        description: 'Max days ahead (e.g., 60)'
        required: false
        type: string
        default: '60'
      
      ai_provider:
        description: 'AI provider (optional)'
        required: false
        type: choice
        options:
          - ''
          - 'duckduckgo'
          - 'bing'
          - 'google'
          - 'ollama'

jobs:
  configure:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: ${{ github.ref }}
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      
      - name: List All Sources
        if: inputs.action == 'List All Sources'
        run: |
          python3 -c "
          import json
          with open('config.prod.json') as f:
              config = json.load(f)
          sources = config.get('scraping', {}).get('sources', [])
          print('\\n=== CONFIGURED SOURCES ===\\n')
          for i, source in enumerate(sources, 1):
              status = '‚úì ENABLED' if source.get('enabled', False) else '‚äò DISABLED'
              print(f'{i}. {source.get(\"name\", \"Unnamed\")} [{status}]')
              print(f'   Type: {source.get(\"type\", \"unknown\")}')
              print(f'   URL: {source.get(\"url\", \"N/A\")}')
              print()
          "
      
      - name: Add New Source
        if: inputs.action == 'Add New Source'
        run: |
          python3 << 'ENDPYTHON'
          import json
          
          # Load config
          with open('config.prod.json', 'r') as f:
              config = json.load(f)
          
          # Create new source
          source = {
              'name': '${{ inputs.source_name }}',
              'type': '${{ inputs.source_type }}',
              'url': '${{ inputs.source_url }}',
              'enabled': '${{ inputs.source_enabled }}' == 'true',
              'options': {}
          }
          
          # Add options if provided
          if '${{ inputs.filter_ads }}' == 'true':
              source['options']['filter_ads'] = True
          
          if '${{ inputs.exclude_keywords }}':
              keywords = [k.strip() for k in '${{ inputs.exclude_keywords }}'.split(',')]
              source['options']['exclude_keywords'] = keywords
          
          if '${{ inputs.max_days_ahead }}':
              source['options']['max_days_ahead'] = int('${{ inputs.max_days_ahead }}')
          
          if '${{ inputs.ai_provider }}':
              source['options']['ai_provider'] = '${{ inputs.ai_provider }}'
          
          # Add to config
          if 'scraping' not in config:
              config['scraping'] = {}
          if 'sources' not in config['scraping']:
              config['scraping']['sources'] = []
          
          config['scraping']['sources'].append(source)
          
          # Save config
          with open('config.prod.json', 'w') as f:
              json.dump(config, f, indent=2)
          
          print(f"‚úì Added source: {source['name']}")
          ENDPYTHON
      
      - name: Test Source
        if: inputs.action == 'Test Source'
        run: |
          python3 << 'ENDPYTHON'
          import json
          import sys
          sys.path.insert(0, 'src')
          
          from modules.scraper import EventScraper
          
          # Load config
          with open('config.prod.json', 'r') as f:
              config = json.load(f)
          
          # Find source by name
          sources = config.get('scraping', {}).get('sources', [])
          source = None
          for s in sources:
              if s.get('name') == '${{ inputs.source_name }}':
                  source = s
                  break
          
          if not source:
              print(f"‚úó Source not found: ${{ inputs.source_name }}")
              sys.exit(1)
          
          # Test scraper
          print(f"üîç Testing: {source['name']}")
          print(f"Type: {source['type']}")
          print(f"URL: {source['url']}")
          
          scraper = EventScraper(config, '.')
          events = scraper.scrape_source(source)
          
          if events:
              print(f"\\n‚úì Success! Found {len(events)} event(s)")
              if events:
                  print("\\nFirst event:")
                  print(f"  Title: {events[0].get('title', 'N/A')}")
                  print(f"  Date: {events[0].get('start_time', 'N/A')}")
          else:
              print("\\n‚ö† No events found")
          ENDPYTHON
      
      - name: Fix Broken Scrapers
        if: inputs.action == 'Fix Broken Scrapers'
        run: |
          python3 << 'ENDPYTHON'
          import json
          import sys
          sys.path.insert(0, 'src')
          
          from modules.scraper import EventScraper
          
          # Load config
          with open('config.prod.json', 'r') as f:
              config = json.load(f)
          
          sources = config.get('scraping', {}).get('sources', [])
          scraper = EventScraper(config, '.')
          
          print("Testing all sources to find broken ones...\\n")
          
          broken = []
          for i, source in enumerate(sources):
              if not source.get('enabled', False):
                  continue
              
              print(f"Testing {source.get('name')}...", end=' ')
              
              try:
                  events = scraper.scrape_source(source)
                  if events is None or len(events) == 0:
                      print("‚ö† No events")
                      broken.append(source.get('name'))
                  else:
                      print(f"‚úì OK ({len(events)} events)")
              except Exception as e:
                  print(f"‚úó FAILED: {e}")
                  broken.append(source.get('name'))
          
          if broken:
              print(f"\\n‚ö† Found {len(broken)} broken source(s):")
              for name in broken:
                  print(f"  - {name}")
              print("\\nPlease review and update these sources manually.")
          else:
              print("\\n‚úì All sources are working!")
          ENDPYTHON
      
      - name: Commit changes
        if: inputs.action == 'Add New Source' || inputs.action == 'Edit Source'
        run: |
          git config user.name "Scraper Config Bot"
          git config user.email "bot@krwl-hof.github.io"
          git add config.prod.json
          git commit -m "feat: configure scraper - ${{ inputs.action }}" || echo "No changes to commit"
          git push
