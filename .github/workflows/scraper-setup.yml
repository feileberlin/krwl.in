name: ðŸ”§ Scraper Setup Tool - Process Configuration

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SCRAPER SETUP TOOL - CI INTEGRATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# PURPOSE:
# Process new scraper configurations created via the drag'n'drop setup tool.
# Validates, tests, and optionally enables new scraper sources.
#
# WHAT THIS WORKFLOW DOES:
# 1. ðŸ” Validates uploaded scraper configuration files
# 2. ðŸ§ª Tests the scraper against the target URL
# 3. âœ… Adds the source to config.json if tests pass
# 4. ðŸ“Š Reports results back to the PR/issue
#
# TRIGGERS:
# - When new configuration files are added to config/scraper_mappings/
# - Manual workflow dispatch for testing
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

on:
  push:
    paths:
      - 'config/scraper_mappings/**_ci_config.json'
    branches:
      - main
      - 'feature/**'
  
  pull_request:
    paths:
      - 'config/scraper_mappings/**_ci_config.json'
  
  workflow_dispatch:
    inputs:
      config_file:
        description: 'Scraper config file to process (e.g., example_com_ci_config.json)'
        type: string
        required: false
      test_only:
        description: 'Only test, do not add to config.json'
        type: boolean
        default: true
        required: false

permissions:
  contents: write
  pull-requests: write

concurrency:
  group: "scraper-setup-${{ github.ref }}"
  cancel-in-progress: true

jobs:
  validate-config:
    name: ðŸ” Validate Configuration
    runs-on: ubuntu-latest
    outputs:
      config_files: ${{ steps.find_configs.outputs.files }}
      has_new_configs: ${{ steps.find_configs.outputs.has_new }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 2
      
      - name: Find new/changed config files
        id: find_configs
        run: |
          # Check for manually specified file
          if [ -n "${{ github.event.inputs.config_file }}" ]; then
            CONFIG_FILE="config/scraper_mappings/${{ github.event.inputs.config_file }}"
            if [ -f "$CONFIG_FILE" ]; then
              echo "files=$CONFIG_FILE" >> $GITHUB_OUTPUT
              echo "has_new=true" >> $GITHUB_OUTPUT
            else
              echo "âŒ Config file not found: $CONFIG_FILE" >> $GITHUB_STEP_SUMMARY
              echo "has_new=false" >> $GITHUB_OUTPUT
            fi
            exit 0
          fi
          
          # Find changed config files
          if [ "${{ github.event_name }}" = "push" ]; then
            CHANGED_FILES=$(git diff --name-only HEAD~1 HEAD -- 'config/scraper_mappings/*_ci_config.json' 2>/dev/null || echo "")
          elif [ "${{ github.event_name }}" = "pull_request" ]; then
            CHANGED_FILES=$(git diff --name-only ${{ github.event.pull_request.base.sha }} ${{ github.sha }} -- 'config/scraper_mappings/*_ci_config.json' 2>/dev/null || echo "")
          else
            CHANGED_FILES=""
          fi
          
          if [ -n "$CHANGED_FILES" ]; then
            echo "files=$CHANGED_FILES" >> $GITHUB_OUTPUT
            echo "has_new=true" >> $GITHUB_OUTPUT
            echo "## ðŸ“„ New Scraper Configurations Found" >> $GITHUB_STEP_SUMMARY
            echo "$CHANGED_FILES" | while read file; do
              echo "- \`$file\`" >> $GITHUB_STEP_SUMMARY
            done
          else
            echo "has_new=false" >> $GITHUB_OUTPUT
            echo "â„¹ï¸ No new scraper configurations found" >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: Validate JSON structure
        if: steps.find_configs.outputs.has_new == 'true'
        run: |
          echo "## ðŸ” Configuration Validation" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          for CONFIG_FILE in ${{ steps.find_configs.outputs.files }}; do
            echo "Validating: $CONFIG_FILE"
            
            # Check if file exists and is valid JSON
            if ! python3 -c "import json; json.load(open('$CONFIG_FILE'))"; then
              echo "âŒ Invalid JSON: \`$CONFIG_FILE\`" >> $GITHUB_STEP_SUMMARY
              exit 1
            fi
            
            # Validate required fields
            export CONFIG_FILE_PATH="$CONFIG_FILE"
            python3 << 'PYEOF'
          import json
          import sys
          import os

          config_file = os.environ.get('CONFIG_FILE_PATH', '')

          with open(config_file) as f:
              config = json.load(f)

          errors = []

          # Check structure
          if 'source' not in config:
              errors.append("Missing 'source' section")
          else:
              if not config['source'].get('name'):
                  errors.append("Missing source name")
              if not config['source'].get('url'):
                  errors.append("Missing source URL")

          if 'field_mappings' not in config:
              errors.append("Missing 'field_mappings' section")
          else:
              required_fields = ['title', 'start_date', 'location_name']
              for field in required_fields:
                  if field not in config['field_mappings']:
                      errors.append(f"Missing required field mapping: {field}")
                  elif not config['field_mappings'][field].get('selector'):
                      errors.append(f"Missing selector for required field: {field}")

          if errors:
              print("âŒ Validation errors:")
              for error in errors:
                  print(f"  - {error}")
              sys.exit(1)
          else:
              print("âœ… Configuration valid!")
          PYEOF
            
            echo "âœ… Valid: \`$CONFIG_FILE\`" >> $GITHUB_STEP_SUMMARY
          done

  test-scraper:
    name: ðŸ§ª Test Scraper
    runs-on: ubuntu-latest
    needs: validate-config
    if: needs.validate-config.outputs.has_new == 'true'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Setup Python environment
        uses: ./.github/actions/setup-python-env
      
      - name: Test scraper configurations
        run: |
          echo "## ðŸ§ª Scraper Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          for CONFIG_FILE in ${{ needs.validate-config.outputs.config_files }}; do
            echo "Testing scraper from: $CONFIG_FILE"
            
            # Run the scraper setup API to test the configuration
            export CONFIG_FILE_PATH="$CONFIG_FILE"
            python3 << 'PYEOF'
          import json
          import sys
          import os
          sys.path.insert(0, 'src')

          from modules.scraper_setup_api import ScraperSetupAPI
          from pathlib import Path

          # Get config file from environment
          config_file = os.environ.get('CONFIG_FILE_PATH', '')

          # Load the configuration
          with open(config_file) as f:
              config = json.load(f)

          source_name = config['source']['name']
          source_url = config['source']['url']

          print(f"Testing scraper for: {source_name}")
          print(f"URL: {source_url}")

          # Initialize API and analyze URL
          api = ScraperSetupAPI(Path('.'))
          analysis = api.analyze_url(source_url)

          if analysis['success']:
              print(f"âœ… URL accessible and analyzed")
              
              # Check if our selectors find elements
              containers = analysis.get('detected_containers', [])
              print(f"  - Found {len(containers)} event containers")
              
              suggestions = analysis.get('suggestions', {})
              for field, mappings in config['field_mappings'].items():
                  selector = mappings.get('selector', '')
                  if selector in str(suggestions.get(field, [])):
                      print(f"  - {field}: âœ… Selector matches")
                  else:
                      print(f"  - {field}: âš ï¸ Selector not verified (may still work)")
          else:
              print(f"âŒ Analysis failed: {analysis.get('error', 'Unknown error')}")
              sys.exit(1)
          PYEOF
            
            if [ $? -eq 0 ]; then
              echo "âœ… **$CONFIG_FILE** - Test passed" >> $GITHUB_STEP_SUMMARY
            else
              echo "âŒ **$CONFIG_FILE** - Test failed" >> $GITHUB_STEP_SUMMARY
            fi
          done

  add-to-config:
    name: âœ… Add Source to Config
    runs-on: ubuntu-latest
    needs: [validate-config, test-scraper]
    if: |
      needs.validate-config.outputs.has_new == 'true' &&
      github.event_name == 'push' &&
      github.ref == 'refs/heads/main' &&
      github.event.inputs.test_only != 'true'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Add sources to config.json
        run: |
          echo "## âœ… Adding Sources to config.json" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          for CONFIG_FILE in ${{ needs.validate-config.outputs.config_files }}; do
            # Store in environment variable so Python can access it
            export CONFIG_FILE_PATH="$CONFIG_FILE"
            python3 << 'PYEOF'
          import json
          import os
          from pathlib import Path

          # Get config file from environment
          config_file = os.environ.get('CONFIG_FILE_PATH', '')

          # Load scraper config
          with open(config_file) as f:
              scraper_config = json.load(f)

          source_name = scraper_config['source']['name']
          source_url = scraper_config['source']['url']

          # Load main config
          with open('config.json') as f:
              main_config = json.load(f)

          # Check if source already exists
          existing_sources = main_config.get('scraping', {}).get('sources', [])
          source_exists = any(s.get('name') == source_name for s in existing_sources)

          if source_exists:
              print(f"â„¹ï¸ Source '{source_name}' already exists in config.json")
          else:
              # Add new source
              new_source = {
                  'name': source_name,
                  'url': source_url,
                  'type': 'html',
                  'mapping_file': config_file,
                  'enabled': True,
                  'notes': 'Added via Scraper Setup Tool'
              }
              
              if 'scraping' not in main_config:
                  main_config['scraping'] = {'sources': []}
              if 'sources' not in main_config['scraping']:
                  main_config['scraping']['sources'] = []
              
              main_config['scraping']['sources'].append(new_source)
              
              # Save updated config
              with open('config.json', 'w') as f:
                  json.dump(main_config, f, indent=2)
              
              print(f"âœ… Added source '{source_name}' to config.json")
          PYEOF
            
            echo "- Added \`$(basename $CONFIG_FILE .json)\` to config.json" >> $GITHUB_STEP_SUMMARY
          done
      
      - name: Commit changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          if git diff --quiet config.json; then
            echo "â„¹ï¸ No changes to commit"
          else
            git add config.json
            git commit -m "ðŸ”§ Add new scraper source(s) from setup tool"
            git push
            
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "âœ… Changes committed and pushed!" >> $GITHUB_STEP_SUMMARY
          fi
