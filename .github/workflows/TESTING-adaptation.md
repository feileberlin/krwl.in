# Workflow Adaptation Testing Guide

This document demonstrates how the unified workflow automatically adapts to changes in scraper.py and config.json.

## Test 1: Baseline - Current Configuration

### Current State
```bash
$ python3 src/event_manager.py scraper-info
```

**Expected Output:**
- 12 enabled sources
- 4 supported source types (rss, html, api, facebook)
- Schedule: 04:00, 16:00 Europe/Berlin
- SmartScraper available: true

### Workflow Behavior
- ‚úÖ Scraping runs on schedule (04:00 and 16:00)
- ‚úÖ All 12 sources are scraped
- ‚úÖ Deploy happens if new events found

---

## Test 2: Disable All Sources

### Change Configuration
Edit `config.json`:
```json
{
  "scraping": {
    "sources": [
      {
        "name": "Wochenmarkt Hof",
        "enabled": false,  // Change from true
        ...
      }
      // Disable all sources
    ]
  }
}
```

### Expected Introspection Output
```bash
$ python3 src/event_manager.py scraper-info
```
```json
{
  "enabled_sources": [],
  "source_count": 0
}
```

### Workflow Adaptation
- ‚úÖ `discover-capabilities` detects 0 sources
- ‚úÖ `scrape-events` job is **SKIPPED** (condition: `source_count != '0'`)
- ‚úÖ No unnecessary scraping attempts
- ‚úÖ Workflow completes successfully (graceful degradation)

**Test Command:**
```bash
# In workflow YAML, job scrape-events has condition:
# if: needs.discover-capabilities.outputs.source_count != '0'
```

---

## Test 3: Add New Source Type

### Add iCal Scraper Method
Edit `src/modules/scraper.py`:

```python
def _scrape_ical(self, source):
    """Scrape events from iCal feed"""
    import icalendar
    response = self._make_request(source['url'])
    cal = icalendar.Calendar.from_ical(response.content)
    events = []
    for component in cal.walk('VEVENT'):
        event = {
            'title': str(component.get('summary')),
            'start_time': component.get('dtstart').dt.isoformat(),
            # ... more fields
        }
        events.append(event)
    return events

def get_supported_source_types(self):
    return ['rss', 'html', 'api', 'facebook', 'ical']  # Add 'ical'
```

### Add iCal Source to Config
Edit `config.json`:
```json
{
  "scraping": {
    "sources": [
      {
        "name": "Calendar Events",
        "url": "https://example.com/events.ics",
        "type": "ical",
        "enabled": true
      }
    ]
  }
}
```

### Expected Introspection Output
```bash
$ python3 src/event_manager.py scraper-info
```
```json
{
  "supported_source_types": [
    "rss", "html", "api", "facebook", "ical"
  ],
  "enabled_sources": [
    {
      "name": "Calendar Events",
      "type": "ical",
      "url": "https://example.com/events.ics"
    }
  ]
}
```

### Workflow Adaptation
- ‚úÖ `discover-capabilities` detects new 'ical' type
- ‚úÖ Workflow runs scraping including new iCal source
- ‚úÖ **NO WORKFLOW FILE CHANGES NEEDED**

---

## Test 4: Change Scraping Schedule

### Modify Schedule
Edit `config.json`:
```json
{
  "scraping": {
    "schedule": {
      "timezone": "Europe/Berlin",
      "times": ["06:00", "12:00", "18:00"]  // Change from ["04:00", "16:00"]
    }
  }
}
```

### Expected Introspection Output
```bash
$ python3 src/event_manager.py scraper-info
```
```json
{
  "schedule": {
    "timezone": "Europe/Berlin",
    "times": ["06:00", "12:00", "18:00"]
  }
}
```

### Workflow Adaptation
- ‚úÖ `discover-capabilities` reads new schedule
- ‚ö†Ô∏è **Note**: GitHub Actions cron is hardcoded in workflow YAML
- üìù **Manual Step Required**: Update cron schedule in workflow file
- üí° **Future Enhancement**: Generate workflow file dynamically

**Current Limitation:**
The cron schedule in the workflow YAML must be manually updated to match config.json. This is a GitHub Actions limitation (cron cannot be dynamic).

**Workaround:**
1. Update `config.json` schedule
2. Update `.github/workflows/website-maintenance.yml` cron times
3. Commit both files together

---

## Test 5: Scraping Libraries Not Installed

### Simulate Missing Libraries
```bash
$ pip uninstall beautifulsoup4 requests
```

### Expected Introspection Output
```bash
$ python3 src/event_manager.py scraper-info
```
```json
{
  "scraping_libraries_installed": false
}
```

### Workflow Adaptation
- ‚úÖ `discover-capabilities` detects libraries not installed
- ‚úÖ `scrape-events` job is **SKIPPED** (condition: `scraping_enabled == 'true'`)
- ‚úÖ Workflow doesn't fail, just skips scraping
- ‚úÖ Deployment jobs still run if triggered manually

---

## Test 6: Manual Workflow Triggers

### Test "info" Task
GitHub Actions ‚Üí Website Maintenance ‚Üí Run workflow ‚Üí Select task: `info`

**Expected Behavior:**
- ‚úÖ `discover-capabilities` runs
- ‚úÖ `show-info` job runs and displays full capabilities
- ‚úÖ All other jobs are skipped
- ‚úÖ No deployment

### Test "scrape-only" Task
GitHub Actions ‚Üí Website Maintenance ‚Üí Run workflow ‚Üí Select task: `scrape-only`

**Expected Behavior:**
- ‚úÖ `discover-capabilities` runs
- ‚úÖ `scrape-events` runs
- ‚úÖ Events are scraped and committed
- ‚úÖ Deployment jobs are **SKIPPED**

### Test "force-deploy" Task
GitHub Actions ‚Üí Website Maintenance ‚Üí Run workflow ‚Üí Select task: `force-deploy`

**Expected Behavior:**
- ‚úÖ `discover-capabilities` runs
- ‚úÖ `full-rebuild` runs (even if no changes)
- ‚úÖ `deploy` runs
- ‚úÖ Site is rebuilt and deployed

---

## Test 7: Push-Triggered Deployment

### Test Event Data Change
```bash
$ git add assets/json/events.json
$ git commit -m "Update events"
$ git push
```

**Expected Workflow Behavior:**
- ‚úÖ Trigger: `on.push.paths` matches `events.json`
- ‚úÖ `update-events` job runs (fast path)
- ‚úÖ `deploy` job runs
- ‚úÖ No full rebuild needed

### Test Scraper Change
```bash
$ git add src/modules/scraper.py
$ git commit -m "Update scraper"
$ git push
```

**Expected Workflow Behavior:**
- ‚úÖ Trigger: `on.push.paths` matches `scraper.py`
- ‚úÖ `full-rebuild` job runs (comprehensive path)
- ‚úÖ `deploy` job runs
- ‚úÖ Complete site regeneration

### Test Config Change
```bash
$ git add config.json
$ git commit -m "Update config"
$ git push
```

**Expected Workflow Behavior:**
- ‚úÖ Trigger: `on.push.paths` matches `config.json`
- ‚úÖ `full-rebuild` job runs
- ‚úÖ `deploy` job runs
- ‚úÖ Site rebuilt with new configuration

---

## Test 8: Concurrent Run Protection

### Test Concurrent Scraping
Trigger workflow manually while scheduled run is in progress.

**Expected Behavior:**
- ‚úÖ Second run waits for first to complete
- ‚úÖ Concurrency group: `website-maintenance`
- ‚úÖ `cancel-in-progress: false` prevents cancellation
- ‚úÖ Both runs complete successfully (no conflicts)

---

## Test 9: Error Handling

### Test Failed Source
Add a source with invalid URL:
```json
{
  "name": "Invalid Source",
  "url": "https://invalid-domain-that-does-not-exist.com",
  "type": "html",
  "enabled": true
}
```

**Expected Behavior:**
- ‚úÖ Scraper handles error gracefully
- ‚úÖ Failed source is logged in scraper
- ‚úÖ Other sources continue scraping
- ‚úÖ Workflow completes successfully
- ‚ö†Ô∏è Summary shows warning about failed source

### Test Git Push Conflict
Simulate concurrent commits to main branch.

**Expected Behavior:**
- ‚úÖ Git pull --rebase handles merge
- ‚úÖ If rebase fails, workflow stops with clear error
- ‚úÖ Manual intervention message shown
- ‚ùå Workflow fails (expected - requires manual resolution)

---

## Test 10: Capability Display

### View in Workflow Summary
Run workflow with `info` task.

**Expected Output in Summary:**
```markdown
## üîç Complete Scraper Capabilities

```json
{
  "supported_source_types": [...],
  "enabled_sources": [...],
  "schedule": {...},
  "smart_scraper_available": true,
  "scraping_libraries_installed": true,
  "methods": {...}
}
```

### Key Information
- Enabled Sources: 12
- Scraping Libraries: true
- Schedule: 04:00, 16:00 (Europe/Berlin)
```

---

## Summary: Adaptation Scenarios

| Scenario | Detects Automatically | Adapts Workflow | Requires Manual Update |
|----------|----------------------|-----------------|------------------------|
| Add/remove sources | ‚úÖ Yes | ‚úÖ Yes | ‚ùå No |
| Change source types | ‚úÖ Yes | ‚úÖ Yes | ‚ùå No |
| Add new scraper methods | ‚úÖ Yes | ‚úÖ Yes | ‚ùå No |
| Change schedule times | ‚úÖ Yes (in summary) | ‚ö†Ô∏è Partial | ‚ö†Ô∏è Yes (cron in YAML) |
| Disable all sources | ‚úÖ Yes | ‚úÖ Yes (skips scraping) | ‚ùå No |
| Missing libraries | ‚úÖ Yes | ‚úÖ Yes (skips scraping) | ‚ùå No |
| Config changes | ‚úÖ Yes | ‚úÖ Yes (triggers rebuild) | ‚ùå No |
| Scraper changes | ‚úÖ Yes | ‚úÖ Yes (triggers rebuild) | ‚ùå No |

---

## Running Tests Locally

### Test Introspection
```bash
# Get current capabilities
python3 src/event_manager.py scraper-info

# Pretty print
python3 src/event_manager.py scraper-info | python3 -m json.tool

# Extract specific field
python3 src/event_manager.py scraper-info | jq '.source_count'
```

### Test Workflow Syntax
```bash
# Validate YAML
python3 -c "import yaml; yaml.safe_load(open('.github/workflows/website-maintenance.yml'))"

# Check job conditions
grep -A 5 "if:" .github/workflows/website-maintenance.yml
```

### Simulate Workflow Logic
```bash
# Get capabilities as workflow would
CAPABILITIES=$(python3 src/event_manager.py scraper-info)
SOURCE_COUNT=$(echo "$CAPABILITIES" | jq -r '.enabled_sources | length')
echo "Source count: $SOURCE_COUNT"

# Check condition
if [ "$SOURCE_COUNT" -ne "0" ]; then
  echo "‚úÖ Scraping would run"
else
  echo "‚ö†Ô∏è Scraping would be skipped"
fi
```

---

## Continuous Testing

Add to your development workflow:

```bash
# Before committing scraper changes
python3 src/event_manager.py scraper-info

# Verify output is valid JSON
python3 src/event_manager.py scraper-info | python3 -m json.tool

# Check workflow syntax
python3 -c "import yaml; yaml.safe_load(open('.github/workflows/website-maintenance.yml'))"
```

---

## Future Enhancements

### Dynamic Cron Scheduling
Generate workflow file with correct cron times based on config.json:

```python
# scripts/generate_workflow.py
import json
import yaml
from datetime import datetime

config = json.load(open('config.json'))
schedule = config['scraping']['schedule']
times = schedule['times']
timezone = schedule['timezone']

# Convert times to UTC cron format
# Generate .github/workflows/website-maintenance.yml
```

### Health Monitoring
Track failed sources over time:

```python
# Add to scraper.py
def get_source_health_stats(self):
    """Return success/failure stats for each source"""
    return {
        'success_rate': 0.95,
        'failed_sources': ['Source A', 'Source B'],
        'last_successful_scrape': '2024-01-01T00:00:00Z'
    }
```

### A/B Testing
Deploy to preview environment first:

```yaml
# Add to workflow
deploy-preview:
  if: github.event.inputs.deploy_mode == 'preview'
  # Deploy to /preview/ path for testing
```
