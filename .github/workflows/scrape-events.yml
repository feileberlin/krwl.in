name: ðŸ”„ Scrape Events & Deploy

on:
  schedule:
    # Run twice daily at 4:00 AM and 4:00 PM in Europe/Berlin timezone (CET/CEST)
    # Note: GitHub Actions uses UTC. These times are calculated for Europe/Berlin:
    # - 4 AM CET/CEST = 3 AM UTC (winter) or 2 AM UTC (summer)
    # - 4 PM CET/CEST = 3 PM UTC (winter) or 2 PM UTC (summer)
    # Using 3 AM and 3 PM UTC as a compromise (closer to winter time)
    #
    # IMPORTANT: When changing timezone or times in config.json, update these cron
    # expressions accordingly. See README for timezone conversion guidance.
    - cron: '0 3 * * *'   # ~4 AM CET (3 AM UTC)
    - cron: '0 15 * * *'  # ~4 PM CET (3 PM UTC)
  workflow_dispatch:
    inputs:
      force_deploy:
        description: 'ðŸš€ Force deployment even if no new events found'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  scrape-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install Python dependencies
        run: |
          echo "Installing scraping dependencies..."
          pip install --upgrade pip
          pip install -r requirements.txt
          echo "âœ“ Dependencies installed"

      - name: Configure Git
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"

      - name: Log schedule configuration
        run: |
          echo "ðŸ“… Reading schedule configuration from config..."
          python3 src/modules/scheduler.py config.prod.json

      - name: Run event scraper
        run: |
          echo "Starting event scraping at $(date)"
          cd src
          python3 main.py scrape
          echo "âœ“ Scraping completed"

      - name: Check for changes
        id: check_changes
        run: |
          if [[ -n $(git status --porcelain) ]]; then
            echo "changes=true" >> $GITHUB_OUTPUT
            echo "âœ“ New events found"
          else
            echo "changes=false" >> $GITHUB_OUTPUT
            echo "â„¹ No new events found"
          fi

      - name: Commit scraped events
        if: steps.check_changes.outputs.changes == 'true'
        run: |
          git add data/
          git commit -m "chore: automated event scraping - $(date -u +"%Y-%m-%d %H:%M UTC")"
          echo "âœ“ Changes committed"

      - name: Generate static site
        if: steps.check_changes.outputs.changes == 'true'
        run: |
          echo "Generating static site files..."
          cd src
          python3 main.py generate
          echo "âœ“ Static site generated"

      - name: Commit generated static files
        if: steps.check_changes.outputs.changes == 'true'
        run: |
          git add static/
          git commit -m "chore: regenerate static site after scraping - $(date -u +"%Y-%m-%d %H:%M UTC")" || echo "No static file changes to commit"
          echo "âœ“ Static files committed (if changed)"

      - name: Push changes
        if: steps.check_changes.outputs.changes == 'true'
        run: |
          git push
          echo "âœ“ Changes pushed to repository"

      - name: Download Leaflet library
        if: steps.check_changes.outputs.changes == 'true'
        run: |
          ./download-libs.sh
          echo "âœ“ Leaflet library downloaded"

      - name: Prepare publish directory
        if: steps.check_changes.outputs.changes == 'true'
        run: |
          set -euo pipefail
          rm -rf publish
          mkdir -p publish
          if [ -d static ]; then
            # copy everything from static into publish root (preserve attributes)
            cp -a static/. publish/ || true
          fi
          # Use production config (optimized for speed, no debugging)
          if [ -f config.prod.json ]; then
            cp config.prod.json publish/config.json
            echo "âœ“ Using production config (optimized for maximum speed)"
          fi
          # include CNAME so custom domain stays configured
          if [ -f CNAME ]; then
            cp CNAME publish/CNAME
            echo "âœ“ CNAME configured for custom domain"
          fi
          # prevent Jekyll processing
          touch publish/.nojekyll
          echo "Production publish directory contents:"
          ls -la publish

      - name: Upload Pages artifact
        if: steps.check_changes.outputs.changes == 'true'
        uses: actions/upload-pages-artifact@v3
        with:
          path: publish

      - name: Deploy to GitHub Pages
        if: steps.check_changes.outputs.changes == 'true'
        uses: actions/deploy-pages@v4

      - name: Summary
        run: |
          echo "## Scraping Run Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Date**: $(date -u +"%Y-%m-%d %H:%M UTC")" >> $GITHUB_STEP_SUMMARY
          if [ "${{ steps.check_changes.outputs.changes }}" == "true" ]; then
            echo "- **Status**: âœ… New events found and deployed" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Status**: â„¹ï¸ No new events found" >> $GITHUB_STEP_SUMMARY
          fi
